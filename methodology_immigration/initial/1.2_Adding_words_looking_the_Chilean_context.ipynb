{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b386fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from twarc_csv import DataFrameConverter\n",
    "from twarc.expansions import ensure_flattened\n",
    "import pandas as pd\n",
    "from twarc_csv import dataframe_converter\n",
    "from twarc_csv import CSVConverter, DataFrameConverter\n",
    "import math\n",
    "import emoji\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from advertools import extract_emoji\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6be53",
   "metadata": {},
   "source": [
    "**Manual Input**: Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2d6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_immigration'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf949fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'initial'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a87016",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DataFrameConverter()\n",
    "data = []\n",
    "with open(path+sep+\"data\"+sep+\"first_search.json\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "df = converter.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d70a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339, 74)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099e6c9",
   "metadata": {},
   "source": [
    "### Obtaining the most common hashtags from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7a7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_retrieve(df):\n",
    "    \"\"\"\n",
    "    df : dataframe of tweets\n",
    "    Description: \n",
    "        The function takes as an object a df of tweets obtained via twarc and \n",
    "        returns a generator object.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for line, id in zip(df['entities.hashtags'], df['id']):\n",
    "        if pd.isna(line):\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        data = json.loads(line)\n",
    "        for hashtag in ensure_flattened(data):\n",
    "            yield [hashtag['tag'], id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f028f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df = pd.DataFrame(list(hash_retrieve(df)),\n",
    "                 columns=['tweet_hashtags', 'id'])\n",
    "\n",
    "hashtags_df = hashtags_df.groupby('id')['tweet_hashtags'].apply(lambda x: ','.join(x))\n",
    "df = df.merge(hashtags_df, how='left', left_on='id', right_on='id')\n",
    "df['tweet_hashtags']=df['tweet_hashtags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c35286",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasht=[]\n",
    "for i in df.index:\n",
    "    try:\n",
    "        hasht.append(df['tweet_hashtags'][i].split(\",\"))\n",
    "    except:\n",
    "        hasht.append([])\n",
    "    \n",
    "    \n",
    "df['list_hashtags']=hasht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f38bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags=[]\n",
    "for i in df.index:\n",
    "    hashtags+=df['list_hashtags'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbfa3",
   "metadata": {},
   "source": [
    "**Look output**: 100 most used hashtag for your topic in tweets geolocated in Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c4166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iquique', 370), ('chile', 247), ('peorpresidentedelahistoria', 247), ('piñera', 227), ('migrantes', 185), ('colchane', 119), ('contigochv', 116), ('inmigrantes', 92), ('antofagasta', 86), ('venezolanos', 82), ('migracion', 67), ('elpeorgobiernodelahistoria', 64), ('debatepresidencial2021', 53), ('nomasinmigrantes', 50), ('venezuela', 46), ('paronacional', 41), ('migración', 40), ('tarapacá', 40), ('desgobierno', 39), ('alerta', 37), ('arica', 37), ('inmigracion', 36), ('noesinmigracionesinvasion', 35), ('izquierdamiserable', 31), ('boric', 27), ('holachilelared', 25), ('nomasdelincuentescolombianos', 24), ('chvnoticias', 23), ('inmigracionilegaldesatada', 23), ('kast', 23), ('migraresunderecho', 22), ('muchogustomega', 21), ('crisismigratoria', 20), ('elecciones2021cl', 20), ('bienvenidos13', 20), ('mesacentral', 20), ('ddhh', 19), ('nomasinmigrantesilegales', 19), ('xenofobia', 19), ('kastpresidente2022', 19), ('iquiquedicebasta', 18), ('tamarugal', 17), ('buenosdiastvn', 17), ('calama', 17), ('despiertaconchv', 17), ('tarapaca', 16), ('tudia13', 15), ('migrante', 15), ('camioneros', 15), ('chilesincomunismo', 15), ('onu', 14), ('conloscamionerosno', 14), ('verguenzanacional', 14), ('piñeraladroncobarde', 14), ('elpeorpresidentedelahistoria', 13), ('sichel', 13), ('fuerailegalesdechile', 13), ('carcelparapiñera', 12), ('santiago', 12), ('inmigración', 12), ('buenosdíastvn', 12), ('cavadachvcnn', 12), ('nomasinmigracionilegal', 12), ('atreveteconkast', 12), ('nuevaleymigraciones', 12), ('peorgobiernodelahistoria', 11), ('ningunserhumanoesilegal', 11), ('pautalibre', 11), ('covid19', 10), ('refugiados', 10), ('venezolanosenchile', 10), ('rut', 10), ('ifelaboral', 10), ('controldefronteras', 10), ('ahora', 9), ('contigoendirectochv', 9), ('maduro', 9), ('crisishumanitaria', 9), ('altohospicio', 9), ('fueraacnurdechile', 9), ('paronacionaldecamioneros', 9), ('nadieesilegal', 9), ('iquiquesos', 9), ('venezolano', 9), ('colombia', 9), ('candidatollegotuhora', 9), ('kastpresidente', 9), ('amor', 8), ('huara', 8), ('peru', 8), ('chilenos', 8), ('puq', 8), ('diainternacionaldelmigrante', 8), ('estadonacional', 8), ('boricpresidente', 8), ('debatechvcnn', 8), ('migraresunderechohumano', 8), ('lavin', 8), ('rechazodesalida2022', 7), ('gobierno', 7)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter(hashtags)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = count.most_common(100)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583c34b",
   "metadata": {},
   "source": [
    "### Obtaining the most common words from the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251fb6a",
   "metadata": {},
   "source": [
    "Here we create a function to clean the text. Removing url, obatining demojized text, removing punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be38d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer   \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "stopwords_eng =stopwords.words(\"english\")\n",
    "stopwords_es =stopwords.words(\"spanish\")\n",
    "stopwords_all=stopwords_eng+stopwords_es\n",
    "\n",
    "def cleanTweets(s,remove_hash):\n",
    "    #Function to clean tweets, for now i am keeping emojis and hashtags. Alternative version\n",
    "    if type(s)==np.float:\n",
    "        return \"\"\n",
    "    #Demojize text\n",
    "#    s=emoji.demojize(s,delimiters=(\"\", \" \"))\n",
    "    \n",
    "    #Remove new lines, etc.\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = s.replace(\"\\\\n\", \" \")\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    s= emoji_pattern.sub(r'', s)\n",
    "    \n",
    "    \n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    #s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    #Remove punctuation    \n",
    "    s = re.sub('[()!?]', ' ', s)\n",
    "    s = re.sub('\\[.*?\\]',' ', s)\n",
    "    s = re.sub('\\[,*?\\]',' ', s)\n",
    "    # custom removals\n",
    "    #s = re.sub(r'@[A-Za-z0-9_]+', \"\", s) # replace mentions\n",
    "    s = re.sub(r':[^:]+','[emoji]',s) # remove demojized text\n",
    "    s= re.sub(r'[0-9]','',s)# remove digits\n",
    "    \n",
    "    s = s.translate(str.maketrans('', '', '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'))\n",
    "    \n",
    "    s=s.lower()\n",
    "    if remove_hash==True:\n",
    "        s = re.sub(r'#[A-Za-z0-9_]+', \"\", s)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    s = ' '.join([word for word in s.split() if word not in stopwords_all])\n",
    "    \n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54139c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-8fa97597cc03>:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if type(s)==np.float:\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "df['cleaned_text'] = [cleanTweets(text,False) for text in df['text']]\n",
    "df['cleaned_text']= df['cleaned_text'].apply(unidecode.unidecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d0164",
   "metadata": {},
   "source": [
    "**Look output**: 100 most used word for your topic in tweets geolocated in Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bffeb1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inmigrantes', 4968), ('migrantes', 3216), ('chile', 2542), ('pais', 1878), ('migracion', 1877), ('inmigracion', 1862), ('q', 1760), ('si', 1657), ('chilenos', 1021), ('gobierno', 948), ('ser', 924), ('ilegales', 900), ('ilegal', 872), ('solo', 868), ('inmigrante', 731), ('delincuentes', 728), ('ahora', 705), ('gente', 704), ('venezolanos', 701), ('asi', 641), ('migrante', 632), ('@sebastianpinera', 587), ('hace', 574), ('pinera', 551), ('hoy', 541), ('norte', 508), ('personas', 504), ('anos', 480), ('iquique', 474), ('delincuencia', 468), ('problema', 461), ('hacer', 451), ('bien', 440), ('casa', 437), ('paises', 434), ('debe', 423), ('mas', 417), ('trabajo', 410), ('mismo', 405), ('puede', 395), ('onu', 375), ('refugiados', 367), ('tema', 363), ('@gabrielboric', 362), ('x', 358), ('#iquique', 357), ('cobarde', 356), ('aca', 350), ('ver', 349), ('va', 344), ('nadie', 341), ('mejor', 329), ('dia', 318), ('menos', 317), ('peor', 316), ('ley', 313), ('@joseantoniokast', 312), ('control', 309), ('extranjeros', 307), ('etc', 306), ('deja', 304), ('boric', 300), ('bachelet', 300), ('mal', 299), ('presidente', 298), ('venezuela', 296), ('izquierda', 295), ('gracias', 291), ('tener', 289), ('siempre', 285), ('da', 282), ('van', 278), ('descontrolada', 277), ('situacion', 276), ('quieren', 275), ('entrar', 268), ('derecho', 266), ('emigrantes', 266), ('tan', 265), ('aqui', 265), ('salud', 260), ('derecha', 260), ('vienen', 260), ('@gobiernodechile', 258), ('parte', 257), ('vez', 257), ('ahi', 257), ('hacen', 255), ('cada', 255), ('frontera', 255), ('kast', 254), ('@t', 252), ('via', 251), ('politicos', 250), ('@biobio', 248), ('creo', 248), ('millones', 243), ('fronteras', 242), ('deben', 238), ('mundo', 236), ('crisis', 235), ('toda', 234), ('#chile', 234), ('favor', 233), ('chileno', 233), ('como', 233), ('politica', 233), ('derechos', 231), ('dice', 230), ('miles', 230), ('despues', 229), ('pasa', 229), ('ademas', 228), ('quiere', 226), ('nunca', 224), ('vida', 223), ('forma', 223), ('ninos', 220), ('culpa', 217), ('gran', 216), ('terrorismo', 212), ('violencia', 212), ('ingreso', 212), ('basta', 211), ('ayuda', 209), ('cosas', 208), ('xenofobia', 208), ('mayoria', 206), ('aun', 204), ('pueblo', 203), ('decir', 202), ('carabineros', 201), ('paso', 200), ('vivir', 197), ('araucania', 196), ('luego', 196), ('problemas', 196), ('recibir', 196), ('verdad', 195), ('ningun', 195), ('hecho', 194), ('acuerdo', 192), ('trabajar', 192), ('todas', 191), ('igual', 191), ('colchane', 191), ('cargo', 190), ('plata', 190), ('maduro', 190), ('mientras', 190), ('santiago', 188), ('llegan', 184), ('pobres', 183), ('sur', 182), ('#migrantes', 180), ('falta', 180), ('pueden', 178), ('viene', 178), ('sido', 178), ('familia', 175), ('verguenza', 174), ('deportacion', 171), ('claro', 171), ('mujeres', 171), ('lleno', 170), ('@mbachelet', 170), ('usted', 169), ('nueva', 168), ('parece', 167), ('dias', 167), ('pobreza', 167), ('dejo', 166), ('cosa', 164), ('bueno', 164), ('mierda', 161), ('haitianos', 161), ('mano', 161), ('trafico', 160), ('cara', 160), ('calles', 158), ('vamos', 157), ('seguridad', 157), ('autoridades', 157), ('apoyo', 155), ('social', 155), ('@meganoticiascl', 155), ('casas', 154), ('narcos', 153), ('respeto', 153), ('expulsion', 152), ('leyes', 152), ('irregular', 152), ('dar', 151), ('hacia', 150), ('tiempo', 147), ('hizo', 147), ('dijo', 147), ('carpas', 146), ('deportados', 146), ('ud', 146)]\n"
     ]
    }
   ],
   "source": [
    "df['cleaned_text']=df['cleaned_text'].fillna(\"\")\n",
    "string=\" \".join(list(df['cleaned_text']))\n",
    "from collections import Counter\n",
    "split_it = string.split()\n",
    "Counter = Counter(split_it)\n",
    "most_occur = Counter.most_common(200)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4007f",
   "metadata": {},
   "source": [
    "### Defining the final keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4330d",
   "metadata": {},
   "source": [
    "**Manual input**: Looking the most common words in chilean context, decide if you want to add new words to the list and put all the words in the list (the words that you used in the first search and also the words that you want to add).\n",
    "  \n",
    "Also add the start and end date for your final data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d82bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words=['inmigracion', 'migracion', 'migrante','migrantes', 'inmigrante', 'inmigrantes', 'emigrantes',\n",
    "'deportacion', 'deportado', 'deportados', 'refugiado', 'refugiados','venezolanos', 'extranjeros', 'xenofobia','haitianos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5f7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'keywords.sav'\n",
    "pickle.dump((list_of_words), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b814afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'start_search_date.sav'\n",
    "start_date= pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'end_search_date.sav'\n",
    "end_date= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597fe3f",
   "metadata": {},
   "source": [
    "**Manual input:** Add words related with your country of interest separated by an OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b41f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_country=\"(chile OR santiago OR iquique OR arica OR piñera OR pinera OR boric OR kast OR valparaiso OR antofagasta OR colchane OR copiapo OR coquimbo OR rancagua OR talca OR conce OR temuco OR puerto montt OR valdivia OR coyhaique OR punta arenas OR weon OR weona)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5357974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'country_string.sav'\n",
    "pickle.dump((str_country), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fae87",
   "metadata": {},
   "source": [
    "Read the output and follow the instruction in case that you have to remove some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90765fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to run the next cell and search in twarc.\n"
     ]
    }
   ],
   "source": [
    "query=\"\"\n",
    "for word in list_of_words:\n",
    "    query+= word + \" OR \"\n",
    "query=query[:len(query)-4]\n",
    "\n",
    "query_place=\"(\"+query+\")\"+ \" -is:retweet place_country:CL\"\n",
    "query_wrelated=\"(\"+query+\")\"+ str_country + \" -is:retweet\"\n",
    "\n",
    "if len(query_place)<=1024 and len(query_wrelated)<=1024:\n",
    "    print(\"Ready to run the next cell and search in twarc.\")\n",
    "else:\n",
    "    print(\"Query too long, reduce the list of words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8b2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+\"second_search.txt\", 'w') as f:\n",
    "    f.write(query_place)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(query_wrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01629",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1efcc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\test\\methodology_immigration\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/2 lines of input file [00:00<?, ?it/s]\n",
      "100%|##########| Processed 2/2 lines of input file [00:22<00:00, 11.07s/it]\n",
      "100%|##########| Processed 2/2 lines of input file [00:48<00:00, 24.06s/it]\n"
     ]
    }
   ],
   "source": [
    "file=path_data+sep+\"second_search.txt\"\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} second_search_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26700b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets to download: 526424\n"
     ]
    }
   ],
   "source": [
    "count=pd.read_csv(path_data+sep+'second_search_count.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5305f",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b66f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\test\\methodology_immigration\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/2 lines of input file [00:00<?, ?it/s]\n",
      "100%|##########| Processed 2/2 lines of input file [16:38<00:00, 499.25s/it]\n",
      "100%|##########| Processed 2/2 lines of input file [4:48:20<00:00, 8650.29s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} second_search.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4514268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
