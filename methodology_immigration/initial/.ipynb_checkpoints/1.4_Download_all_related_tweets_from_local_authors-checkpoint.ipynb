{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f383a2",
   "metadata": {},
   "source": [
    "## Downloading all the related tweets from our final list of authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05120480",
   "metadata": {},
   "source": [
    "Now that we cleaned our authors lefting mostly chilean ones, we will go to download all the tweets from this authors that are related with our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32f7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc00879",
   "metadata": {},
   "source": [
    "**Manual input:** Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87121871",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_immigration'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353643a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'initial'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ae408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'keywords.sav'\n",
    "list_of_words=pickle.load(open(file, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'start_search_date.sav'\n",
    "start_date= pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'end_search_date.sav'\n",
    "end_date= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69887674",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'filtered_by_location.sav'\n",
    "df = pickle.load(open(file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c372e9",
   "metadata": {},
   "source": [
    "Here we obtain the final list of authors to look for all the topic related tweets from these authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b818972",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=list(df['author.username'])\n",
    "authors = list(set(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1040d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\n",
    "for word in list_of_words:\n",
    "    query+= word + \" OR \"\n",
    "query=query[:len(query)-4]\n",
    "\n",
    "query=\"(\"+query+\")\" + \"-is:retweet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3ec1c",
   "metadata": {},
   "source": [
    "We create a list of queries with the keywords and the authors. Considering that the maximum lenght of a query is 1,024 characters, we should create a list of queries, each one with less than 1,024 characters. After, with the option searches we run from a txt file all the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3318fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text= query\n",
    "queries_list=[]\n",
    "query_from_good=\"\"\n",
    "query_from_test=\"\"\n",
    "\n",
    "for author in authors:\n",
    "    query_from_test+=\"from: \" + author + \" OR \"\n",
    "    if len(query_text + query_from_test) + 3 <= 1024:\n",
    "        query_from_good = query_from_test\n",
    "    else:\n",
    "        queries_list.append(\"(\"+query_from_good[:len(query_from_good)-4]+\")\"+ query_text)\n",
    "        query_from_good=\"from: \" + author + \" OR \"\n",
    "        query_from_test=\"from: \" + author + \" OR \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489cb0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(from: JJurgersen OR from: SebaQb2 OR from: enjoythevibee OR from: INpopuli OR from: marlenemusa OR from: Felipe68390452 OR from: EugenPoch OR from: felivald OR from: paulo_otarola OR from: revista_paula OR from: oscararriagada9 OR from: Sedbastian OR from: Andrea34704450 OR from: bsty10 OR from: RojasOtarola OR from: julio_cesar669 OR from: kikayfelipe OR from: Axl95735941 OR from: del_tipo OR from: AlfredoAtfta OR from: zamoralive OR from: luisguillermo00 OR from: antoniova63 OR from: RZuzuni OR from: Carlogic0 OR from: Polo_Rubbersoul OR from: matterlandsen OR from: sandreoli43 OR from: hotlinezer0 OR from: PaolaAraneda OR from: MiguelSH96 OR from: bprokurica OR from: Danielnewen OR from: Freddy61797909 OR from: mgraciabt OR from: Alfonso15788812 OR from: HelmutKramer)(inmigracion OR migracion OR migrante OR migrantes OR inmigrante OR inmigrantes OR emigrantes OR deportacion OR deportado OR deportados OR refugiado OR refugiados OR venezolanos OR extranjeros OR xenofobia OR haitianos)-is:retweet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfca2e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b77d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+'final_search_for_tweets_df.txt', 'w') as f:\n",
    "    for line in queries_list:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276b622",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=path_data+sep+'final_search_for_tweets_df.txt'\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} final_search_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41600c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=pd.read_csv(path_data+sep+'final_search_count.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b00cef",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} final_corpus_search.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
