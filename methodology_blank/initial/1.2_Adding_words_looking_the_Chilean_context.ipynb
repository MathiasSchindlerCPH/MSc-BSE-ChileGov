{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b386fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from twarc_csv import DataFrameConverter\n",
    "from twarc.expansions import ensure_flattened\n",
    "import pandas as pd\n",
    "from twarc_csv import dataframe_converter\n",
    "from twarc_csv import CSVConverter, DataFrameConverter\n",
    "import math\n",
    "import emoji\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from advertools import extract_emoji\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6be53",
   "metadata": {},
   "source": [
    "**Manual Input**: Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_blank'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf949fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'initial'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a87016",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DataFrameConverter()\n",
    "data = []\n",
    "with open(path+sep+\"data\"+sep+\"first_search.json\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "df = converter.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d70a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_retrieve(df):\n",
    "    \"\"\"\n",
    "    df : dataframe of tweets\n",
    "    Description: \n",
    "        The function takes as an object a df of tweets obtained via twarc and \n",
    "        returns a generator object.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for line, id in zip(df['entities.hashtags'], df['id']):\n",
    "        if pd.isna(line):\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        data = json.loads(line)\n",
    "        for hashtag in ensure_flattened(data):\n",
    "            yield [hashtag['tag'], id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f028f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_df = pd.DataFrame(list(hash_retrieve(df)),\n",
    "                 columns=['tweet_hashtags', 'id'])\n",
    "\n",
    "hashtags_df = hashtags_df.groupby('id')['tweet_hashtags'].apply(lambda x: ','.join(x))\n",
    "df = df.merge(hashtags_df, how='left', left_on='id', right_on='id')\n",
    "df['tweet_hashtags']=df['tweet_hashtags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c35286",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasht=[]\n",
    "for i in df.index:\n",
    "    try:\n",
    "        hasht.append(df['tweet_hashtags'][i].split(\",\"))\n",
    "    except:\n",
    "        hasht.append([])\n",
    "    \n",
    "    \n",
    "df['list_hashtags']=hasht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f38bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags=[]\n",
    "for i in df.index:\n",
    "    hashtags+=df['list_hashtags'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbfa3",
   "metadata": {},
   "source": [
    "**Look output**: 100 most used hashtag for your topic in tweets geolocated in Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter(hashtags)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = count.most_common(100)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer   \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "stopwords_eng =stopwords.words(\"english\")\n",
    "stopwords_es =stopwords.words(\"spanish\")\n",
    "stopwords_all=stopwords_eng+stopwords_es\n",
    "\n",
    "def cleanTweets(s,remove_hash):\n",
    "    #Function to clean tweets, for now i am keeping emojis and hashtags. Alternative version\n",
    "    if type(s)==np.float:\n",
    "        return \"\"\n",
    "    #Demojize text\n",
    "#    s=emoji.demojize(s,delimiters=(\"\", \" \"))\n",
    "    \n",
    "    #Remove new lines, etc.\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = s.replace(\"\\\\n\", \" \")\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    s= emoji_pattern.sub(r'', s)\n",
    "    \n",
    "    \n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    #s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    #Remove punctuation    \n",
    "    s = re.sub('[()!?]', ' ', s)\n",
    "    s = re.sub('\\[.*?\\]',' ', s)\n",
    "    s = re.sub('\\[,*?\\]',' ', s)\n",
    "    # custom removals\n",
    "    #s = re.sub(r'@[A-Za-z0-9_]+', \"\", s) # replace mentions\n",
    "    s = re.sub(r':[^:]+','[emoji]',s) # remove demojized text\n",
    "    s= re.sub(r'[0-9]','',s)# remove digits\n",
    "    \n",
    "    s = s.translate(str.maketrans('', '', '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'))\n",
    "    \n",
    "    s=s.lower()\n",
    "    if remove_hash==True:\n",
    "        s = re.sub(r'#[A-Za-z0-9_]+', \"\", s)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    s = ' '.join([word for word in s.split() if word not in stopwords_all])\n",
    "    \n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54139c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "df['cleaned_text'] = [cleanTweets(text,False) for text in df['text']]\n",
    "df['cleaned_text']= df['cleaned_text'].apply(unidecode.unidecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d0164",
   "metadata": {},
   "source": [
    "**Look output**: 100 most used word for your topic in tweets geolocated in Chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffeb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text']=df['cleaned_text'].fillna(\"\")\n",
    "string=\" \".join(list(df['cleaned_text']))\n",
    "from collections import Counter\n",
    "split_it = string.split()\n",
    "Counter = Counter(split_it)\n",
    "most_occur = Counter.most_common(200)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4330d",
   "metadata": {},
   "source": [
    "**Manual input**: Looking the most common words in chilean context, decide if you want to add new words to the list and put all the words in the list (the words that you used in the first search and also the words that you want to add).\n",
    "  \n",
    "Also add the start and end date for your final data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d82bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words=['word1','word2','word3','word4','word5','word6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'keywords.sav'\n",
    "pickle.dump((list_of_words), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'start_search_date.sav'\n",
    "start_date= pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'end_search_date.sav'\n",
    "end_date= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597fe3f",
   "metadata": {},
   "source": [
    "**Manual input:** Add words related with your country of interest separated by an OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b41f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_country=\"(country_related_word_1 OR country_related_word_2 OR country_related_word_3)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_data+sep+'country_string.sav'\n",
    "pickle.dump((str_country), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fae87",
   "metadata": {},
   "source": [
    "Read the output and follow the instruction in case that you have to remove some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90765fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\n",
    "for word in list_of_words:\n",
    "    query+= word + \" OR \"\n",
    "query=query[:len(query)-4]\n",
    "\n",
    "query_place=\"(\"+query+\")\"+ \" -is:retweet place_country:CL\"\n",
    "query_wrelated=\"(\"+query+\")\"+ str_country + \" -is:retweet\"\n",
    "\n",
    "if len(query_place)<=1024 and len(query_wrelated)<=1024:\n",
    "    print(\"Ready to run the next cell and search in twarc.\")\n",
    "else:\n",
    "    print(\"Query too long, reduce the list of words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+\"second_search.txt\", 'w') as f:\n",
    "    f.write(query_place)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(query_wrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01629",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=path_data+sep+\"second_search.txt\"\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} second_search_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26700b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=pd.read_csv(path_data+sep+'second_search_count.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5305f",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} second_search.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4514268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
