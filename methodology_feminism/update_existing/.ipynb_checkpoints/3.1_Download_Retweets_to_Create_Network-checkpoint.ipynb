{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7998fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a64aa",
   "metadata": {},
   "source": [
    "**Manual input:** Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e967e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_feminism'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37df21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'udpated_existing'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1493cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'keywords.sav'\n",
    "list_of_words=pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'start_update_date.sav'\n",
    "start_date=pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'end_update_date.sav'\n",
    "end_date=pickle.load(open(file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4841bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = path_data+sep+'final_data_set.sav'\n",
    "df = pickle.load(open(file, 'rb'))\n",
    "authors=list(df['author.username'])\n",
    "authors = list(set(authors))\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99759dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\n",
    "for word in list_of_words:\n",
    "    query+= word + \" OR \"\n",
    "query=query[:len(query)-4]\n",
    "query=\"(\"+query+\")\"+ \" is:retweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04eb9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text= query\n",
    "queries_list=[]\n",
    "query_from_good=\"\"\n",
    "query_from_test=\"\"\n",
    "\n",
    "for author in authors:\n",
    "    query_from_test+=\"from: \" + author + \" OR \"\n",
    "    if len(query_text + query_from_test) + 3 <= 1024:\n",
    "        query_from_good = query_from_test\n",
    "    else:\n",
    "        queries_list.append(\"(\"+query_from_good[:len(query_from_good)-4]+\")\"+ query_text)\n",
    "        query_from_good=\"from: \" + author + \" OR \"\n",
    "        query_from_test=\"from: \" + author + \" OR \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248eb076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(queries_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbda2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+'search_for_network_upd.txt', 'w') as f:\n",
    "    for line in queries_list:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f634a",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bb9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/23 lines of input file [00:00<?, ?it/s]\n",
      "  9%|8         | Processed 2/23 lines of input file [00:01<00:14,  1.41it/s]\n",
      " 13%|#3        | Processed 3/23 lines of input file [00:02<00:18,  1.06it/s]\n",
      " 17%|#7        | Processed 4/23 lines of input file [00:03<00:20,  1.07s/it]\n",
      " 22%|##1       | Processed 5/23 lines of input file [00:05<00:20,  1.13s/it]\n",
      " 26%|##6       | Processed 6/23 lines of input file [00:06<00:20,  1.19s/it]\n",
      " 30%|###       | Processed 7/23 lines of input file [00:07<00:19,  1.21s/it]\n",
      " 35%|###4      | Processed 8/23 lines of input file [00:09<00:18,  1.23s/it]\n",
      " 39%|###9      | Processed 9/23 lines of input file [00:10<00:17,  1.25s/it]\n",
      " 43%|####3     | Processed 10/23 lines of input file [00:11<00:16,  1.26s/it]\n",
      " 48%|####7     | Processed 11/23 lines of input file [00:12<00:15,  1.26s/it]\n",
      " 52%|#####2    | Processed 12/23 lines of input file [00:15<00:17,  1.61s/it]\n",
      " 57%|#####6    | Processed 13/23 lines of input file [00:16<00:15,  1.50s/it]\n",
      " 61%|######    | Processed 14/23 lines of input file [00:17<00:12,  1.43s/it]\n",
      " 65%|######5   | Processed 15/23 lines of input file [00:19<00:11,  1.38s/it]\n",
      " 70%|######9   | Processed 16/23 lines of input file [00:20<00:10,  1.52s/it]\n",
      " 74%|#######3  | Processed 17/23 lines of input file [00:22<00:08,  1.44s/it]\n",
      " 78%|#######8  | Processed 18/23 lines of input file [00:23<00:06,  1.39s/it]\n",
      " 83%|########2 | Processed 19/23 lines of input file [00:24<00:05,  1.42s/it]\n",
      " 87%|########6 | Processed 20/23 lines of input file [00:26<00:04,  1.37s/it]\n",
      " 91%|#########1| Processed 21/23 lines of input file [00:27<00:02,  1.34s/it]\n",
      " 96%|#########5| Processed 22/23 lines of input file [00:28<00:01,  1.32s/it]\n",
      "100%|##########| Processed 23/23 lines of input file [00:30<00:00,  1.33s/it]\n",
      "100%|##########| Processed 23/23 lines of input file [00:31<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "file=path_data+sep+'search_for_network_upd.txt'\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} rt_count_upd.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab429f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets to download: 245\n"
     ]
    }
   ],
   "source": [
    "count=pd.read_csv(path_data+sep+'rt_count_upd.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cef22f",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d41f921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/23 lines of input file [00:00<?, ?it/s]\n",
      "  9%|8         | Processed 2/23 lines of input file [00:01<00:17,  1.20it/s]\n",
      " 13%|#3        | Processed 3/23 lines of input file [00:03<00:22,  1.14s/it]\n",
      " 17%|#7        | Processed 4/23 lines of input file [00:04<00:23,  1.26s/it]\n",
      " 22%|##1       | Processed 5/23 lines of input file [00:06<00:24,  1.37s/it]\n",
      " 26%|##6       | Processed 6/23 lines of input file [00:08<00:26,  1.53s/it]\n",
      " 30%|###       | Processed 7/23 lines of input file [00:09<00:26,  1.63s/it]\n",
      " 35%|###4      | Processed 8/23 lines of input file [00:11<00:23,  1.57s/it]\n",
      " 39%|###9      | Processed 9/23 lines of input file [00:12<00:21,  1.56s/it]\n",
      " 43%|####3     | Processed 10/23 lines of input file [00:14<00:20,  1.57s/it]\n",
      " 48%|####7     | Processed 11/23 lines of input file [00:16<00:18,  1.55s/it]\n",
      " 52%|#####2    | Processed 12/23 lines of input file [00:17<00:16,  1.53s/it]\n",
      " 57%|#####6    | Processed 13/23 lines of input file [00:19<00:16,  1.61s/it]\n",
      " 61%|######    | Processed 14/23 lines of input file [00:20<00:14,  1.60s/it]\n",
      " 65%|######5   | Processed 15/23 lines of input file [00:22<00:13,  1.63s/it]\n",
      " 70%|######9   | Processed 16/23 lines of input file [00:24<00:11,  1.57s/it]\n",
      " 74%|#######3  | Processed 17/23 lines of input file [00:25<00:09,  1.55s/it]\n",
      " 78%|#######8  | Processed 18/23 lines of input file [00:27<00:07,  1.54s/it]\n",
      " 83%|########2 | Processed 19/23 lines of input file [00:28<00:06,  1.63s/it]\n",
      " 87%|########6 | Processed 20/23 lines of input file [00:30<00:04,  1.65s/it]\n",
      " 91%|#########1| Processed 21/23 lines of input file [00:32<00:03,  1.62s/it]\n",
      " 96%|#########5| Processed 22/23 lines of input file [00:33<00:01,  1.57s/it]\n",
      "100%|##########| Processed 23/23 lines of input file [00:34<00:00,  1.50s/it]\n",
      "100%|##########| Processed 23/23 lines of input file [00:36<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} rt_for_network_upd.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d9bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
