{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49293ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76969ce",
   "metadata": {},
   "source": [
    "**Manual Input**: Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e063b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_feminism'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c547e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'initial'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dffa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'final_data_set.sav'\n",
    "df = pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'left_hashtags_for_label.sav'\n",
    "left_hashtags = pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'right_hashtags_for_label.sav'\n",
    "right_hashtags= pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'start_date_for_label.sav'\n",
    "start_date= pickle.load(open(file, 'rb'))\n",
    "\n",
    "file = path_data+sep+'end_date_for_label.sav'\n",
    "end_date= pickle.load(open(file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90dca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_string=\"(\"\n",
    "for hashtag in (left_hashtags+right_hashtags):\n",
    "    hashtag_string+=hashtag + \" OR \"\n",
    "hashtag_string=hashtag_string[:len(hashtag_string)-4]+\")\"\n",
    "    \n",
    "authors=list(df['author.username'])\n",
    "authors = list(set(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca2f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text= hashtag_string\n",
    "queries_list=[]\n",
    "query_from_good=\"\"\n",
    "query_from_test=\"\"\n",
    "\n",
    "for author in authors:\n",
    "    query_from_test+=\"from: \" + author + \" OR \"\n",
    "    if len(query_text + query_from_test) + 3 <= 1024:\n",
    "        query_from_good = query_from_test\n",
    "    else:\n",
    "        queries_list.append(\"(\"+query_from_good[:len(query_from_good)-4]+\")\"+ query_text)\n",
    "        query_from_good=\"from: \" + author + \" OR \"\n",
    "        query_from_test=\"from: \" + author + \" OR \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4a7abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(queries_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14b99e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(from: Gonzalo00333993 OR from: mozeifala OR from: Sandraprd OR from: gloriagarciai OR from: PATY_CHILELIBRE OR from: alejandroleivaz OR from: SernamEGAntofa OR from: L_FelipeG OR from: ClaudiaDides OR from: eddu_tar OR from: misabel_diaz OR from: elisadaskal OR from: SoleVerito1 OR from: FelidaeFerox OR from: FeministasRDRM OR from: kari_munoz OR from: Respirologo OR from: PDI_Antofagasta OR from: cristianhcd OR from: etoledog)(boricpresidente OR seguimos OR boricpresidentedechile OR apruebodignidad OR rutaesperanzaxboric OR boricnosune OR boricpresidente2022 OR unmillondepuertasxboric OR 1millondepuertasxboric OR meunoconboric OR ahorayasna OR boricpresidentedechile2022 OR boricenprimeravuelta OR vota1 OR paravivirmejor OR atreveteconkast OR kastpresidente2022 OR kastpresidente OR vota2votakast OR atreveteporchile OR sepuede OR todochileconkast OR kastledaesperanzaachile OR chilevotakast OR atrevidos OR consichelsepuede OR sichelpresidente OR atrevidosporkast OR votakast OR mujeresporkast)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f15728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+'search_for_labels.txt', 'w') as f:\n",
    "    for line in queries_list:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c1413",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c29f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/37 lines of input file [00:00<?, ?it/s]\n",
      "  5%|5         | Processed 2/37 lines of input file [00:02<00:48,  1.39s/it]\n",
      "  8%|8         | Processed 3/37 lines of input file [00:05<01:06,  1.95s/it]\n",
      " 11%|#         | Processed 4/37 lines of input file [00:08<01:13,  2.21s/it]\n",
      " 14%|#3        | Processed 5/37 lines of input file [00:10<01:15,  2.35s/it]\n",
      " 16%|#6        | Processed 6/37 lines of input file [00:13<01:16,  2.47s/it]\n",
      " 19%|#8        | Processed 7/37 lines of input file [00:16<01:15,  2.50s/it]\n",
      " 22%|##1       | Processed 8/37 lines of input file [00:18<01:13,  2.52s/it]\n",
      " 24%|##4       | Processed 9/37 lines of input file [00:21<01:11,  2.55s/it]\n",
      " 27%|##7       | Processed 10/37 lines of input file [00:24<01:11,  2.63s/it]\n",
      " 30%|##9       | Processed 11/37 lines of input file [00:26<01:08,  2.62s/it]\n",
      " 32%|###2      | Processed 12/37 lines of input file [00:29<01:05,  2.62s/it]\n",
      " 35%|###5      | Processed 13/37 lines of input file [00:32<01:03,  2.66s/it]\n",
      " 38%|###7      | Processed 14/37 lines of input file [00:34<01:00,  2.63s/it]\n",
      " 41%|####      | Processed 15/37 lines of input file [00:37<00:57,  2.61s/it]\n",
      " 43%|####3     | Processed 16/37 lines of input file [00:39<00:55,  2.62s/it]\n",
      " 46%|####5     | Processed 17/37 lines of input file [00:42<00:52,  2.61s/it]\n",
      " 49%|####8     | Processed 18/37 lines of input file [00:44<00:49,  2.59s/it]\n",
      " 51%|#####1    | Processed 19/37 lines of input file [00:47<00:47,  2.62s/it]\n",
      " 54%|#####4    | Processed 20/37 lines of input file [00:50<00:44,  2.63s/it]\n",
      " 57%|#####6    | Processed 21/37 lines of input file [00:52<00:42,  2.63s/it]\n",
      " 59%|#####9    | Processed 22/37 lines of input file [00:55<00:39,  2.65s/it]\n",
      " 62%|######2   | Processed 23/37 lines of input file [00:58<00:37,  2.65s/it]\n",
      " 65%|######4   | Processed 24/37 lines of input file [01:00<00:34,  2.62s/it]\n",
      " 68%|######7   | Processed 25/37 lines of input file [01:03<00:31,  2.60s/it]\n",
      " 70%|#######   | Processed 26/37 lines of input file [01:05<00:28,  2.57s/it]\n",
      " 73%|#######2  | Processed 27/37 lines of input file [01:08<00:25,  2.56s/it]\n",
      " 76%|#######5  | Processed 28/37 lines of input file [01:10<00:23,  2.57s/it]\n",
      " 78%|#######8  | Processed 29/37 lines of input file [01:13<00:20,  2.56s/it]\n",
      " 81%|########1 | Processed 30/37 lines of input file [01:16<00:17,  2.57s/it]\n",
      " 84%|########3 | Processed 31/37 lines of input file [01:18<00:15,  2.56s/it]\n",
      " 86%|########6 | Processed 32/37 lines of input file [01:21<00:12,  2.55s/it]\n",
      " 89%|########9 | Processed 33/37 lines of input file [01:23<00:10,  2.55s/it]\n",
      " 92%|#########1| Processed 34/37 lines of input file [01:26<00:07,  2.59s/it]\n",
      " 95%|#########4| Processed 35/37 lines of input file [01:29<00:05,  2.65s/it]\n",
      " 97%|#########7| Processed 36/37 lines of input file [01:31<00:02,  2.62s/it]\n",
      "100%|##########| Processed 37/37 lines of input file [01:34<00:00,  2.59s/it]\n",
      "100%|##########| Processed 37/37 lines of input file [01:36<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "file=path_data+sep+'search_for_labels.txt'\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} labels_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d468bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets to download: 52953\n"
     ]
    }
   ],
   "source": [
    "count=pd.read_csv(path_data+sep+'labels_count.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089202c0",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac75cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/37 lines of input file [00:00<?, ?it/s]\n",
      "  5%|5         | Processed 2/37 lines of input file [00:30<08:45, 15.01s/it]\n",
      "  8%|8         | Processed 3/37 lines of input file [01:25<18:03, 31.86s/it]\n",
      " 11%|#         | Processed 4/37 lines of input file [03:29<36:25, 66.23s/it]\n",
      " 14%|#3        | Processed 5/37 lines of input file [04:21<32:41, 61.29s/it]\n",
      " 16%|#6        | Processed 6/37 lines of input file [05:02<28:08, 54.46s/it]\n",
      " 19%|#8        | Processed 7/37 lines of input file [05:30<23:02, 46.08s/it]\n",
      " 22%|##1       | Processed 8/37 lines of input file [06:05<20:30, 42.42s/it]\n",
      " 24%|##4       | Processed 9/37 lines of input file [06:22<16:08, 34.58s/it]\n",
      " 27%|##7       | Processed 10/37 lines of input file [07:48<22:40, 50.39s/it]\n",
      " 30%|##9       | Processed 11/37 lines of input file [08:27<20:20, 46.93s/it]\n",
      " 32%|###2      | Processed 12/37 lines of input file [08:52<16:44, 40.20s/it]\n",
      " 35%|###5      | Processed 13/37 lines of input file [09:06<12:55, 32.31s/it]\n",
      " 38%|###7      | Processed 14/37 lines of input file [09:23<10:40, 27.84s/it]\n",
      " 41%|####      | Processed 15/37 lines of input file [10:25<14:00, 38.22s/it]\n",
      " 43%|####3     | Processed 16/37 lines of input file [11:05<13:29, 38.53s/it]\n",
      " 46%|####5     | Processed 17/37 lines of input file [11:19<10:25, 31.29s/it]\n",
      " 49%|####8     | Processed 18/37 lines of input file [12:11<11:51, 37.43s/it]\n",
      " 51%|#####1    | Processed 19/37 lines of input file [12:25<09:08, 30.46s/it]\n",
      " 54%|#####4    | Processed 20/37 lines of input file [16:44<28:05, 99.13s/it]\n",
      " 57%|#####6    | Processed 21/37 lines of input file [16:51<19:04, 71.50s/it]\n",
      " 59%|#####9    | Processed 22/37 lines of input file [17:16<14:21, 57.42s/it]\n",
      " 62%|######2   | Processed 23/37 lines of input file [17:51<11:48, 50.63s/it]\n",
      " 65%|######4   | Processed 24/37 lines of input file [18:16<09:17, 42.91s/it]\n",
      " 68%|######7   | Processed 25/37 lines of input file [18:40<07:29, 37.43s/it]\n",
      " 70%|#######   | Processed 26/37 lines of input file [19:17<06:49, 37.27s/it]\n",
      " 73%|#######2  | Processed 27/37 lines of input file [19:34<05:10, 31.07s/it]\n",
      " 76%|#######5  | Processed 28/37 lines of input file [20:30<05:46, 38.46s/it]\n",
      " 78%|#######8  | Processed 29/37 lines of input file [21:17<05:29, 41.17s/it]\n",
      " 81%|########1 | Processed 30/37 lines of input file [21:36<04:02, 34.63s/it]\n",
      " 84%|########3 | Processed 31/37 lines of input file [21:59<03:05, 30.91s/it]\n",
      " 86%|########6 | Processed 32/37 lines of input file [22:08<02:02, 24.46s/it]\n",
      " 89%|########9 | Processed 33/37 lines of input file [22:56<02:06, 31.55s/it]\n",
      " 92%|#########1| Processed 34/37 lines of input file [23:34<01:40, 33.38s/it]\n",
      " 95%|#########4| Processed 35/37 lines of input file [24:06<01:05, 32.98s/it]\n",
      " 97%|#########7| Processed 36/37 lines of input file [24:43<00:34, 34.26s/it]\n",
      "100%|##########| Processed 37/37 lines of input file [25:24<00:00, 36.38s/it]\n",
      "100%|##########| Processed 37/37 lines of input file [26:47<00:00, 43.46s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} right_left_hashtags.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a8f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
