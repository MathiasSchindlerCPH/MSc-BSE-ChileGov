{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f383a2",
   "metadata": {},
   "source": [
    "## Downloading all the related tweets from our final list of authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05120480",
   "metadata": {},
   "source": [
    "Now that we cleaned our authors lefting mostly chilean ones, we will go to download all the tweets from this authors that are related with our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32f7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc00879",
   "metadata": {},
   "source": [
    "**Manual input:** Set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87121871",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/acoub/OneDrive/Desktop/DSDM/Thesis/ChileGov/methodology_feminism'\n",
    "sep='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353643a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_scripts = path+sep+'initial'\n",
    "path_data= path+sep+'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ae408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'keywords.sav'\n",
    "list_of_words=pickle.load(open(file, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'start_search_date.sav'\n",
    "start_date= pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename = path_data+sep+'end_search_date.sav'\n",
    "end_date= pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69887674",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path_data+sep+'filtered_by_location.sav'\n",
    "df = pickle.load(open(file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b818972",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=list(df['author.username'])\n",
    "authors = list(set(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1040d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\n",
    "for word in list_of_words:\n",
    "    query+= word + \" OR \"\n",
    "query=query[:len(query)-4]\n",
    "\n",
    "query=\"(\"+query+\")\" +\" -is:retweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3318fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text= query\n",
    "queries_list=[]\n",
    "query_from_good=\"\"\n",
    "query_from_test=\"\"\n",
    "\n",
    "for author in authors:\n",
    "    query_from_test+=\"from: \" + author + \" OR \"\n",
    "    if len(query_text + query_from_test) + 3 <= 1024:\n",
    "        query_from_good = query_from_test\n",
    "    else:\n",
    "        queries_list.append(\"(\"+query_from_good[:len(query_from_good)-4]+\")\"+ query_text)\n",
    "        query_from_good=\"from: \" + author + \" OR \"\n",
    "        query_from_test=\"from: \" + author + \" OR \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489cb0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(from: Jovitos OR from: snapymolly OR from: LegadoBachelet OR from: joblancoo OR from: Info_War_Chile OR from: Hocicon9Hocicon OR from: EMMERGEai OR from: BetsySolis77 OR from: VMLEIVA OR from: diarioafta OR from: ClauAvilaBravo OR from: ppdmujeres OR from: rositapaez OR from: latotov OR from: RodrigoArayaC_ OR from: mrtgroffteith OR from: cbrautigaml OR from: Gorc_3 OR from: sofintinaok OR from: yaracerpa OR from: roxyaceved OR from: ThiareSalinas OR from: JavieraPaz32 OR from: MalbaBarahona OR from: Sanfeliu OR from: crisisstian OR from: monse_candiar OR from: MunicipioValpo OR from: Yuyi_Lo OR from: CIFChile OR from: supermarketero OR from: tiagobiskupovic OR from: mono_dex OR from: NachoAlcaino OR from: AdrianaEGomez OR from: FranSolar OR from: lvera50 OR from: drarogers OR from: funadite OR from: madero_cl OR from: AvecesCalla OR from: data_int OR from: Votamos__Todos OR from: Laura_en_twiter)(feminismo OR feminista OR feministas OR sexismo OR sexista OR machismo OR sororidad OR genero) -is:retweet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b77d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data+sep+'final_search_for_tweets_df.txt', 'w') as f:\n",
    "    for line in queries_list:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276b622",
   "metadata": {},
   "source": [
    "**Manual revision:** Run the following two cells to know the total number of tweets to download and see if does not exceed the limit of your twarc acount. If is OK run also the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220d2faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/18 lines of input file [00:00<?, ?it/s]\n",
      " 11%|#1        | Processed 2/18 lines of input file [00:01<00:11,  1.35it/s]\n",
      " 17%|#6        | Processed 3/18 lines of input file [00:02<00:14,  1.03it/s]\n",
      " 22%|##2       | Processed 4/18 lines of input file [00:04<00:16,  1.16s/it]\n",
      " 28%|##7       | Processed 5/18 lines of input file [00:05<00:15,  1.20s/it]\n",
      " 33%|###3      | Processed 6/18 lines of input file [00:06<00:14,  1.24s/it]\n",
      " 39%|###8      | Processed 7/18 lines of input file [00:08<00:13,  1.26s/it]\n",
      " 44%|####4     | Processed 8/18 lines of input file [00:09<00:12,  1.27s/it]\n",
      " 50%|#####     | Processed 9/18 lines of input file [00:10<00:11,  1.28s/it]\n",
      " 56%|#####5    | Processed 10/18 lines of input file [00:12<00:10,  1.29s/it]\n",
      " 61%|######1   | Processed 11/18 lines of input file [00:13<00:09,  1.31s/it]\n",
      " 67%|######6   | Processed 12/18 lines of input file [00:14<00:08,  1.37s/it]\n",
      " 72%|#######2  | Processed 13/18 lines of input file [00:16<00:06,  1.36s/it]\n",
      " 78%|#######7  | Processed 14/18 lines of input file [00:17<00:05,  1.37s/it]\n",
      " 83%|########3 | Processed 15/18 lines of input file [00:19<00:04,  1.37s/it]\n",
      " 89%|########8 | Processed 16/18 lines of input file [00:20<00:02,  1.37s/it]\n",
      " 94%|#########4| Processed 17/18 lines of input file [00:21<00:01,  1.43s/it]\n",
      "100%|##########| Processed 18/18 lines of input file [00:23<00:00,  1.48s/it]\n",
      "100%|##########| Processed 18/18 lines of input file [00:24<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "file=path_data+sep+'final_search_for_tweets_df.txt'\n",
    "%cd {path_data}\n",
    "! twarc2 searches --archive --counts-only --granularity day --start-time {start_date} --end-time {end_date} {file} final_search_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41600c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets to download: 1642\n"
     ]
    }
   ],
   "source": [
    "count=pd.read_csv(path_data+sep+'final_search_count.csv',encoding = \"ISO-8859-1\")\n",
    "print(\"Tweets to download: \"+str(count['day_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b00cef",
   "metadata": {},
   "source": [
    "**Download the tweets:** (Only if you have enought limit in your twarc account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d86fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acoub\\OneDrive\\Desktop\\DSDM\\Thesis\\Methodology\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | Processed 0/18 lines of input file [00:00<?, ?it/s]\n",
      " 11%|#1        | Processed 2/18 lines of input file [00:04<00:36,  2.29s/it]\n",
      " 17%|#6        | Processed 3/18 lines of input file [00:06<00:33,  2.21s/it]\n",
      " 22%|##2       | Processed 4/18 lines of input file [00:11<00:44,  3.15s/it]\n",
      " 28%|##7       | Processed 5/18 lines of input file [00:14<00:39,  3.01s/it]\n",
      " 33%|###3      | Processed 6/18 lines of input file [00:18<00:40,  3.40s/it]\n",
      " 39%|###8      | Processed 7/18 lines of input file [00:20<00:34,  3.10s/it]\n",
      " 44%|####4     | Processed 8/18 lines of input file [00:24<00:32,  3.29s/it]\n",
      " 50%|#####     | Processed 9/18 lines of input file [00:26<00:27,  3.02s/it]\n",
      " 56%|#####5    | Processed 10/18 lines of input file [00:29<00:23,  2.89s/it]\n",
      " 61%|######1   | Processed 11/18 lines of input file [00:31<00:18,  2.71s/it]\n",
      " 67%|######6   | Processed 12/18 lines of input file [00:34<00:16,  2.69s/it]\n",
      " 72%|#######2  | Processed 13/18 lines of input file [00:39<00:16,  3.25s/it]\n",
      " 78%|#######7  | Processed 14/18 lines of input file [00:41<00:12,  3.17s/it]\n",
      " 83%|########3 | Processed 15/18 lines of input file [00:44<00:09,  3.03s/it]\n",
      " 89%|########8 | Processed 16/18 lines of input file [00:47<00:05,  2.91s/it]\n",
      " 94%|#########4| Processed 17/18 lines of input file [00:51<00:03,  3.36s/it]\n",
      "100%|##########| Processed 18/18 lines of input file [00:54<00:00,  3.14s/it]\n",
      "100%|##########| Processed 18/18 lines of input file [00:56<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd {path_data}\n",
    "! twarc2 searches --archive --start-time {start_date} --end-time {end_date} {file} final_corpus_search.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
